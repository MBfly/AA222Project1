import numpy as np
import matplotlib.pyplot as plt

class Counter:
    def __init__(self):
        self.calls = 0

    def increment(self, k):
        self.calls += k

    def get(self):
        return self.calls

def make_wrapped_functions(f, g, counter):
    # Wrap f to count one evaluation per call.
    def f_wrapped(x):
        counter.increment(1)
        return f(x)
    # Wrap g to count two evaluations per call.
    def g_wrapped(x):
        counter.increment(2)
        return g(x)
    return f_wrapped, g_wrapped

def optimize_with_history(f, g, x0, n, count, prob):
    """
    Modified optimizer that records the convergence history.
    f, g: wrapped functions (so that count() is updated on each call)
    x0: initial guess (numpy array)
    n: maximum allowed function evaluations
    count: function that returns current count (budget used)
    prob: problem name (unused here but provided for compatibility)
    
    Returns:
        x_best: best solution found
        history: list of tuples (iteration index, f(x_best)) for each outer iteration
    """
    history = []
    x_best = np.copy(x0)
    # Record initial value (using f as computed by the wrapper)
    fx0 = f(x_best)
    history.append((0, fx0))
    
    tol = 1e-6       # Tolerance for the norm of gradient
    alpha_init = 1.4 # Initial step length
    c = 1e-4         # Armijo condition parameter
    iter_count = 0
    
    while count() < n:
        # Check whether calling g (cost 2) is allowed.
        if count() + 2 > n:
            break
        
        grad = g(x_best)
        grad_norm = np.linalg.norm(grad)
        # Stop if the gradient is small
        if grad_norm < tol:
            break
        
        # Check that one more f evaluation is allowed.
        if count() + 1 > n:
            break
        fx = f(x_best)
        
        # Backtracking line search initialization
        t = alpha_init
        while True:
            if count() + 1 > n:
                return x_best, history
            x_candidate = x_best - t * grad
            f_candidate = f(x_candidate)
            
            # Check Armijo condition
            if f_candidate <= fx - c * t * np.dot(grad, grad):
                break
            t *= 0.23  # Reduce step length
        
        # Update current point with the candidate
        x_best = x_candidate
        iter_count += 1
        history.append((iter_count, f_candidate))
    return x_best, history


# Rosenbrock
def rosenbrock(x):
    # f(x,y) = (1 - x)^2 + 100*(y - x^2)^2
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

def rosenbrock_grad(x):
    dfdx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)
    dfdy = 200*(x[1] - x[0]**2)
    return np.array([dfdx, dfdy])

# Himmelblau
def himmelblau(x):
    # f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2
    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2

def himmelblau_grad(x):
    dfdx = 4*x[0]*(x[0]**2 + x[1] - 11) + 2*(x[0] + x[1]**2 - 7)
    dfdy = 2*(x[0]**2 + x[1] - 11) + 4*x[1]*(x[0] + x[1]**2 - 7)
    return np.array([dfdx, dfdy])

# Powell's function
def powell(x):
    # f(x) = (x1+10*x2)^2 + 5*(x3-x4)^2 + (x2-2*x3)^4 + 10*(x1-x4)^4
    return (x[0] + 10*x[1])**2 + 5*(x[2] - x[3])**2 + (x[1] - 2*x[2])**4 + 10*(x[0] - x[3])**4

def powell_grad(x):
    # Derivatives computed term by term.
    dfdx1 = 2*(x[0] + 10*x[1]) + 40*(x[0] - x[3])**3
    dfdx2 = 20*(x[0] + 10*x[1]) + 4*(x[1] - 2*x[2])**3
    dfdx3 = 10*(x[2] - x[3]) - 8*(x[1] - 2*x[2])**3
    dfdx4 = -10*(x[2] - x[3]) - 40*(x[0] - x[3])**3
    return np.array([dfdx1, dfdx2, dfdx3, dfdx4])

# Rosenbrock and Himmelblau
init_points_2d = [
    np.array([-1.2, 1.0]),
    np.array([0.0, 0.0]),
    np.array([2.0, 2.0])
]

# Powell
init_points_4d = [
    np.array([-1.2, 1.0, -1.2, 1.0]),
    np.array([0.0, 0.0, 0.0, 0.0]),
    np.array([2.0, 2.0, 2.0, 2.0])
]


def run_optimization(f, g, x0, max_evals, prob):
    counter = Counter()
    f_wrapped, g_wrapped = make_wrapped_functions(f, g, counter)
    x_best, history = optimize_with_history(f_wrapped, g_wrapped, x0, max_evals, counter.get, prob)
    return x_best, history


def plot_history(histories, title, labels):
    plt.figure()
    for hist, lab in zip(histories, labels):
        iters, fvals = zip(*hist)
        plt.plot(iters, fvals, marker='o', label=lab)
    plt.xlabel("Iterations")
    plt.ylabel("Function value")
    plt.title(title)
    plt.legend()
    plt.grid(True)

# Rosenbrock
rosen_histories = []
rosen_labels = []
for i, x0 in enumerate(init_points_2d):
    _, history = run_optimization(rosenbrock, rosenbrock_grad, x0, 20, prob='Rosenbrock')
    rosen_histories.append(history)
    rosen_labels.append(f"{x0}")

# Himmelblau
himmel_histories = []
himmel_labels = []
for i, x0 in enumerate(init_points_2d):
    _, history = run_optimization(himmelblau, himmelblau_grad, x0, 40, prob='Himmelblau')
    himmel_histories.append(history)
    himmel_labels.append(f"{x0}")

# Powell
powell_histories = []
powell_labels = []
for i, x0 in enumerate(init_points_4d):
    _, history = run_optimization(powell, powell_grad, x0, 100, prob='Powell')
    powell_histories.append(history)
    powell_labels.append(f"{x0}")

# Plot convergence plots.
plot_history(rosen_histories, "Convergence Plot: Rosenbrock", rosen_labels)
plot_history(himmel_histories, "Convergence Plot: Himmelblau", himmel_labels)
plot_history(powell_histories, "Convergence Plot: Powell", powell_labels)

plt.show()
